{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'tweet', 'emojis']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Add these lines at the beginning of your code\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "abbreviations = {\n",
    "    'u': 'you',\n",
    "    'r': 'are',\n",
    "    'lol': 'laugh out loud',\n",
    "    'omg': 'oh my god',\n",
    "    'np': 'no problem',\n",
    "    'wtf': 'what the fuck',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'lmfao': 'laughing my fucking ass off',\n",
    "    'idk': 'I don\\'t know',\n",
    "    'btw': 'by the way',\n",
    "    'jk': 'just kidding',\n",
    "    'tbh': 'to be honest',\n",
    "    'fyi': 'for your information',\n",
    "    'imo': 'in my opinion',\n",
    "    'imho': 'in my humble opinion',\n",
    "    'nvm': 'never mind',\n",
    "    'thx': 'thanks',\n",
    "    'tks': 'thanks',\n",
    "    'plz': 'please',\n",
    "    'w/': 'with',\n",
    "    'w/o': 'without',\n",
    "    'b/c': 'because',\n",
    "    'wbu': 'what about you',\n",
    "    'w/e': 'whatever',\n",
    "    'gr8': 'great',\n",
    "    'tho': 'though',\n",
    "    'ppl': 'people',\n",
    "    'fml': 'fuck my life',\n",
    "    'nbd': 'no big deal',\n",
    "    'bff': 'best friends forever',\n",
    "    'ikr': 'I know, right?',\n",
    "    'smh': 'shaking my head',\n",
    "    'irl': 'in real life',\n",
    "    'ik': 'I know',\n",
    "    'sry': 'sorry',\n",
    "    'thn': 'then',\n",
    "    'k': 'okay',\n",
    "    'wbu': 'what about you',\n",
    "    'ic': 'I see',\n",
    "    'afaik': 'as far as I know',\n",
    "    'brb': 'be right back',\n",
    "    'g2g': 'got to go',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'hmu': 'hit me up',\n",
    "    'fomo': 'fear of missing out',\n",
    "    'imy': 'I miss you',\n",
    "    'tb': 'text back',\n",
    "    'g1': 'good one',\n",
    "    'w8': 'wait',\n",
    "    'meh': 'whatever',\n",
    "    'rly': 'really',\n",
    "    'cya': 'see ya',\n",
    "    'gtg': 'got to go',\n",
    "    'ttys': 'talk to you soon',\n",
    "    'l8r': 'later',\n",
    "    'nm': 'nothing much',\n",
    "    'cu': 'see you',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'b4': 'before',\n",
    "    'afk': 'away from keyboard',\n",
    "    'tmi': 'too much information',\n",
    "    'ty': 'thank you',\n",
    "    'njoy': 'enjoy',\n",
    "    'tho': 'though',\n",
    "    'f2f': 'face to face',\n",
    "    'idc': 'I don\\'t care',\n",
    "    'fyi': 'for your information',\n",
    "    'wth': 'what the hell',\n",
    "    'yolo': 'you only live once',\n",
    "    'gratz': 'congratulations',\n",
    "    'gr8': 'great',\n",
    "    'ily': 'I love you',\n",
    "    'thks': 'thanks',\n",
    "    'luv': 'love',\n",
    "}\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # 3. Removing mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # 4. Removing hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # 4. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 5. Removing emojis\n",
    "    tokens = [token for token in tokens if not any(c in emoji.EMOJI_DATA for c in token)]\n",
    "\n",
    "    # 6. Removing special characters and punctuation\n",
    "    tokens = [re.sub(r'\\W+|\\d+', '', token) for token in tokens]\n",
    "\n",
    "    # 7. Expanding contractions and abbreviations using contractions package\n",
    "    tokens = [contractions.fix(token) for token in tokens]\n",
    "\n",
    "    # 8. Replace abbreviations\n",
    "    tokens = [abbreviations.get(token, token) for token in tokens]\n",
    "\n",
    "    # 9. Removing stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # 10. Lemmatization or stemming\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # 11. Remove non-English words\n",
    "    # english_words = set(words.words())\n",
    "    # tokens = [token for token in tokens if token in english_words]\n",
    "    \n",
    "    # Remove empty strings\n",
    "    tokens = [token for token in tokens if token != '']\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tweet = \"Here's an example, tweet with @mentions, http://urls.com, #hashtags and üòÉ emojis!\"\n",
    "preprocessed_tweet = preprocess_tweet(tweet)\n",
    "print(preprocessed_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antanas/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# tweets = pd.read_csv('/Users/antanas/GitRepo/SentPred/Data/TweetsCOV19.csv', index_col=0, dtype={'TweetId': str, 'Username': str,\n",
    "#                                                                                                  'Timestamp': str, 'NoFollowers': int,\n",
    "#                                                                                                  'NoFriends': int, 'NoRetweets': int,\n",
    "#                                                                                                  'NoFavorites': int, 'Entities': str,\n",
    "#                                                                                                  'Sentiment': str, 'Mentions': str,\n",
    "#                                                                                                  'Hashtags': str, 'URLs': str,\n",
    "#                                                                                                  'TweetText': str, 'UserLocation': str})\n",
    "# tweets = pd.read_csv('/Users/antanas/GitRepo/SentPred/Data/TweetsCOV19.csv', index_col=1)[['TweetText', 'Sentiment']]\n",
    "tweets = pd.read_csv('/Users/antanas/GitRepo/SentPred/Data/TweetsCOV19.csv', index_col=1)\n",
    "tweets.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TweetId\n",
       "1178793230223183872                  Los Angeles, CA\n",
       "1178798309491822592                   United Kingdom\n",
       "1178806730358059008                        Las Vegas\n",
       "1178807288171130880                              irl\n",
       "1178809851843006464                   Beijing, China\n",
       "1178818100478656512                   Wilmington, NC\n",
       "1178820929419395072                          Jupiter\n",
       "1178824204688318464                        Hong Kong\n",
       "1178825630605029376                    Baltimore, MD\n",
       "1178833466202279936                  California, USA\n",
       "1178839076235042816                    Virginia, USA\n",
       "1178839418267811846               Est. July 27, 2019\n",
       "1178840722323066881                            Vegas\n",
       "1178840968109416448                 South Beloit, IL\n",
       "1178847160579678208    21 | she/they | poc | 17+ | ü¶à\n",
       "1178850984136589312                           Canada\n",
       "1178853896636321792                               No\n",
       "1178853917607620608                         Sydney^^\n",
       "1178854639044890624              Southern California\n",
       "1178864789252136960              Cowboy & Steak Land\n",
       "Name: UserLocation, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['UserLocation'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv('/Users/antanas/GitRepo/SentPred/Data/bonus_cov19_sa.csv', index_col=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antanas/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "Processing locations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 448727/448727 [21:38:15<00:00,  5.76it/s]   \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feather does not support serializing <class 'pandas.core.indexes.base.Index'> for the index; you can .reset_index() to make the index into column(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de77bb93b5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0musa_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"United States\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_feather\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeather_format\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_feather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m         \u001b[0mto_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m     @doc(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mto_feather\u001b[0;34m(df, path, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInt64Index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;34mf\"feather does not support serializing {typ} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;34m\"for the index; you can .reset_index() to make the index into column(s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feather does not support serializing <class 'pandas.core.indexes.base.Index'> for the index; you can .reset_index() to make the index into column(s)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Nominatim API \n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "# Load the cache dictionary and DataFrame from disk if they exist\n",
    "cache_path = './cache.pkl'\n",
    "df_path = './tweets_processed.feather'\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        cache = pickle.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "if os.path.exists(df_path):\n",
    "    tweets = pd.read_feather(df_path)\n",
    "else:\n",
    "    tweets = pd.read_csv('/Users/antanas/GitRepo/SentPred/Data/TweetsCOV19.csv', index_col=1)\n",
    "    tweets.dropna(inplace=True)\n",
    "    pass\n",
    "\n",
    "def country_state_locator(location):\n",
    "    if location in cache:\n",
    "        return cache[location]\n",
    "    \n",
    "    try:\n",
    "        # Geolocate the location\n",
    "        loc = geolocator.geocode(location, exactly_one=True)\n",
    "        # Split the address by comma\n",
    "        address_components = loc.address.split(\",\")\n",
    "        # The last element is typically the country\n",
    "        country = address_components[-1].strip()\n",
    "        # The second to last is typically the state\n",
    "        state = address_components[-2].strip() if len(address_components) > 1 else None\n",
    "        \n",
    "        cache[location] = (country, state)\n",
    "        \n",
    "        # Save the cache to disk every 100 iterations\n",
    "        if len(cache) % 1000 == 0:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(cache, f)\n",
    "                \n",
    "        return country, state\n",
    "    except (GeocoderTimedOut, GeocoderUnavailable):\n",
    "        cache[location] = (None, None)\n",
    "        return None, None\n",
    "    except:\n",
    "        cache[location] = (None, None)\n",
    "        return None, None\n",
    "\n",
    "# Assuming df is your DataFrame and 'location' is the location column\n",
    "tqdm.pandas(desc=\"Processing locations\")  # This line initializes tqdm with pandas\n",
    "tweets[['country', 'state']] = tweets['UserLocation'].progress_apply(lambda loc: pd.Series(country_state_locator(loc)))\n",
    "\n",
    "# Save the cache and DataFrame to disk at the end\n",
    "with open(cache_path, 'wb') as f:\n",
    "    pickle.dump(cache, f)\n",
    "\n",
    "tweets.to_feather(df_path)\n",
    "\n",
    "usa_tweets = tweets[tweets['country'] == \"United States\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Export to pickle\n",
    "with open('tweets_countries_states.pkl', 'wb') as f:\n",
    "    pickle.dump(tweets, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle\n",
    "with open('tweets_countries_states.pkl', 'rb') as f:\n",
    "    tweets_t = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_tweets = tweets[tweets['country'] == \"United States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Username</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>NoFollowers</th>\n",
       "      <th>NoFriends</th>\n",
       "      <th>NoRetweets</th>\n",
       "      <th>NoFavorites</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>URLs</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>UserLocation</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweetId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.178793e+18</th>\n",
       "      <td>0</td>\n",
       "      <td>bf05d1888dd974fa4a8679c25e2ead03</td>\n",
       "      <td>2019-09-30 22:06:21+00:00</td>\n",
       "      <td>5018.0</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vaccine:Vaccine:-2.6651530673745762;anti vaxxe...</td>\n",
       "      <td>2 -1</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>https://goo.gl/fb/uoeiPk:-:</td>\n",
       "      <td>From my blog: Californians support vaccine law...</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.178807e+18</th>\n",
       "      <td>2</td>\n",
       "      <td>6c9c28a4e449e0fe0d4d2ff449b8d6bc</td>\n",
       "      <td>2019-09-30 23:00:00+00:00</td>\n",
       "      <td>237005.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ied:Improvised_explosive_device:-1.76224648952...</td>\n",
       "      <td>2 -3</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>https://www.fox5vegas.com/toby-keith-and-a-vet...</td>\n",
       "      <td>While serving in Afghanistan in 2010, Marine C...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>United States</td>\n",
       "      <td>Nevada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.178818e+18</th>\n",
       "      <td>5</td>\n",
       "      <td>229b01058fe308c050445b0d2106d7e6</td>\n",
       "      <td>2019-09-30 23:45:11+00:00</td>\n",
       "      <td>37245.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>cold front:Cold_front:-1.3939860168378515;</td>\n",
       "      <td>2 -1</td>\n",
       "      <td>null;</td>\n",
       "      <td>scwx ncwx</td>\n",
       "      <td>https://www.weather.gov/ilm/:-:</td>\n",
       "      <td>Looking like it may be a fall-like weekend com...</td>\n",
       "      <td>Wilmington, NC</td>\n",
       "      <td>United States</td>\n",
       "      <td>North Carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.178821e+18</th>\n",
       "      <td>6</td>\n",
       "      <td>3e422ee40c2780ced96ba7696d67ba92</td>\n",
       "      <td>2019-09-30 23:56:25+00:00</td>\n",
       "      <td>317.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nigga:Nigga:-2.8874080256180297;</td>\n",
       "      <td>3 -1</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>i stopped caring what niggas think when i real...</td>\n",
       "      <td>Jupiter</td>\n",
       "      <td>United States</td>\n",
       "      <td>33458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.178826e+18</th>\n",
       "      <td>8</td>\n",
       "      <td>974df4f4955d2952a2276bd38c564b5a</td>\n",
       "      <td>2019-10-01 00:15:06+00:00</td>\n",
       "      <td>629.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>what just happened:What_Just_Happened_%282008_...</td>\n",
       "      <td>1 -1</td>\n",
       "      <td>kie_vs_theworld</td>\n",
       "      <td>RAW</td>\n",
       "      <td>null;</td>\n",
       "      <td>I hold @kie_vs_theworld personally responsible...</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.265844e+18</th>\n",
       "      <td>11781</td>\n",
       "      <td>b52ddc217a661db2d4c729149cb93c5f</td>\n",
       "      <td>2020-05-28 03:13:35+00:00</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bill gates:Bill_Gates:-0.852067793274493;take ...</td>\n",
       "      <td>2 -2</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>Bahahahaha...Is it ‚ÄúMask On Monday‚Äù or ‚ÄúTake I...</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tennessee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.266524e+18</th>\n",
       "      <td>11789</td>\n",
       "      <td>8245b0bc05f6fbc97f1bf6376e3eb006</td>\n",
       "      <td>2020-05-30 00:17:38+00:00</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1524.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>er:ER_%28TV_series%29:-2.5283896631239986;</td>\n",
       "      <td>3 -4</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>now, he's his normal self but as per our Vet w...</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>United States</td>\n",
       "      <td>North Carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.258013e+18</th>\n",
       "      <td>11797</td>\n",
       "      <td>eb28b233e5039b1b30142896ccea3834</td>\n",
       "      <td>2020-05-06 12:36:00+00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>racist:Racism:-2.7489182698143284;</td>\n",
       "      <td>2 -3</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>Hey spaz, were you calling Trump a racist when...</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>United States</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.261418e+18</th>\n",
       "      <td>11805</td>\n",
       "      <td>2066652744987178573fce8b5953e325</td>\n",
       "      <td>2020-05-15 22:06:35+00:00</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>psychoanalysis:Psychoanalysis:-1.7095004037976...</td>\n",
       "      <td>2 -4</td>\n",
       "      <td>DorseyFilm KyleClark</td>\n",
       "      <td>null;</td>\n",
       "      <td>null;</td>\n",
       "      <td>@DorseyFilm @KyleClark So it's selfish of me t...</td>\n",
       "      <td>Colorado, USA</td>\n",
       "      <td>United States</td>\n",
       "      <td>Colorado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.256049e+18</th>\n",
       "      <td>11807</td>\n",
       "      <td>26de63369cbaa88341aa7455321bd0c1</td>\n",
       "      <td>2020-05-01 02:31:36+00:00</td>\n",
       "      <td>54609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>sars cov 2:Severe_acute_respiratory_syndrome_c...</td>\n",
       "      <td>1 -1</td>\n",
       "      <td>null;</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>https://biorxiv.org/cgi/content/short/2020.04....</td>\n",
       "      <td>Spike mutation pipeline reveals the emergence ...</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166481 rows √ó 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unnamed: 0                          Username  \\\n",
       "TweetId                                                     \n",
       "1.178793e+18          0  bf05d1888dd974fa4a8679c25e2ead03   \n",
       "1.178807e+18          2  6c9c28a4e449e0fe0d4d2ff449b8d6bc   \n",
       "1.178818e+18          5  229b01058fe308c050445b0d2106d7e6   \n",
       "1.178821e+18          6  3e422ee40c2780ced96ba7696d67ba92   \n",
       "1.178826e+18          8  974df4f4955d2952a2276bd38c564b5a   \n",
       "...                 ...                               ...   \n",
       "1.265844e+18      11781  b52ddc217a661db2d4c729149cb93c5f   \n",
       "1.266524e+18      11789  8245b0bc05f6fbc97f1bf6376e3eb006   \n",
       "1.258013e+18      11797  eb28b233e5039b1b30142896ccea3834   \n",
       "1.261418e+18      11805  2066652744987178573fce8b5953e325   \n",
       "1.256049e+18      11807  26de63369cbaa88341aa7455321bd0c1   \n",
       "\n",
       "                              Timestamp  NoFollowers  NoFriends  NoRetweets  \\\n",
       "TweetId                                                                       \n",
       "1.178793e+18  2019-09-30 22:06:21+00:00       5018.0     1933.0         0.0   \n",
       "1.178807e+18  2019-09-30 23:00:00+00:00     237005.0      559.0         2.0   \n",
       "1.178818e+18  2019-09-30 23:45:11+00:00      37245.0      299.0         7.0   \n",
       "1.178821e+18  2019-09-30 23:56:25+00:00        317.0      272.0         0.0   \n",
       "1.178826e+18  2019-10-01 00:15:06+00:00        629.0     2864.0         1.0   \n",
       "...                                 ...          ...        ...         ...   \n",
       "1.265844e+18  2020-05-28 03:13:35+00:00       1261.0     1661.0         0.0   \n",
       "1.266524e+18  2020-05-30 00:17:38+00:00       1080.0     1524.0         1.0   \n",
       "1.258013e+18  2020-05-06 12:36:00+00:00          7.0       91.0         0.0   \n",
       "1.261418e+18  2020-05-15 22:06:35+00:00       4590.0     4920.0         1.0   \n",
       "1.256049e+18  2020-05-01 02:31:36+00:00      54609.0        0.0        25.0   \n",
       "\n",
       "              NoFavorites                                           Entities  \\\n",
       "TweetId                                                                        \n",
       "1.178793e+18          0.0  vaccine:Vaccine:-2.6651530673745762;anti vaxxe...   \n",
       "1.178807e+18          3.0  ied:Improvised_explosive_device:-1.76224648952...   \n",
       "1.178818e+18         12.0         cold front:Cold_front:-1.3939860168378515;   \n",
       "1.178821e+18          0.0                   nigga:Nigga:-2.8874080256180297;   \n",
       "1.178826e+18          1.0  what just happened:What_Just_Happened_%282008_...   \n",
       "...                   ...                                                ...   \n",
       "1.265844e+18          0.0  bill gates:Bill_Gates:-0.852067793274493;take ...   \n",
       "1.266524e+18          9.0         er:ER_%28TV_series%29:-2.5283896631239986;   \n",
       "1.258013e+18          0.0                 racist:Racism:-2.7489182698143284;   \n",
       "1.261418e+18          3.0  psychoanalysis:Psychoanalysis:-1.7095004037976...   \n",
       "1.256049e+18         38.0  sars cov 2:Severe_acute_respiratory_syndrome_c...   \n",
       "\n",
       "             Sentiment              Mentions   Hashtags  \\\n",
       "TweetId                                                   \n",
       "1.178793e+18      2 -1                 null;      null;   \n",
       "1.178807e+18      2 -3                 null;      null;   \n",
       "1.178818e+18      2 -1                 null;  scwx ncwx   \n",
       "1.178821e+18      3 -1                 null;      null;   \n",
       "1.178826e+18      1 -1       kie_vs_theworld        RAW   \n",
       "...                ...                   ...        ...   \n",
       "1.265844e+18      2 -2                 null;      null;   \n",
       "1.266524e+18      3 -4                 null;      null;   \n",
       "1.258013e+18      2 -3                 null;      null;   \n",
       "1.261418e+18      2 -4  DorseyFilm KyleClark      null;   \n",
       "1.256049e+18      1 -1                 null;    bioRxiv   \n",
       "\n",
       "                                                           URLs  \\\n",
       "TweetId                                                           \n",
       "1.178793e+18                        https://goo.gl/fb/uoeiPk:-:   \n",
       "1.178807e+18  https://www.fox5vegas.com/toby-keith-and-a-vet...   \n",
       "1.178818e+18                    https://www.weather.gov/ilm/:-:   \n",
       "1.178821e+18                                              null;   \n",
       "1.178826e+18                                              null;   \n",
       "...                                                         ...   \n",
       "1.265844e+18                                              null;   \n",
       "1.266524e+18                                              null;   \n",
       "1.258013e+18                                              null;   \n",
       "1.261418e+18                                              null;   \n",
       "1.256049e+18  https://biorxiv.org/cgi/content/short/2020.04....   \n",
       "\n",
       "                                                      TweetText  \\\n",
       "TweetId                                                           \n",
       "1.178793e+18  From my blog: Californians support vaccine law...   \n",
       "1.178807e+18  While serving in Afghanistan in 2010, Marine C...   \n",
       "1.178818e+18  Looking like it may be a fall-like weekend com...   \n",
       "1.178821e+18  i stopped caring what niggas think when i real...   \n",
       "1.178826e+18  I hold @kie_vs_theworld personally responsible...   \n",
       "...                                                         ...   \n",
       "1.265844e+18  Bahahahaha...Is it ‚ÄúMask On Monday‚Äù or ‚ÄúTake I...   \n",
       "1.266524e+18  now, he's his normal self but as per our Vet w...   \n",
       "1.258013e+18  Hey spaz, were you calling Trump a racist when...   \n",
       "1.261418e+18  @DorseyFilm @KyleClark So it's selfish of me t...   \n",
       "1.256049e+18  Spike mutation pipeline reveals the emergence ...   \n",
       "\n",
       "                 UserLocation        country           state  \n",
       "TweetId                                                       \n",
       "1.178793e+18  Los Angeles, CA  United States      California  \n",
       "1.178807e+18        Las Vegas  United States          Nevada  \n",
       "1.178818e+18   Wilmington, NC  United States  North Carolina  \n",
       "1.178821e+18          Jupiter  United States           33458  \n",
       "1.178826e+18    Baltimore, MD  United States        Maryland  \n",
       "...                       ...            ...             ...  \n",
       "1.265844e+18        Tennessee  United States       Tennessee  \n",
       "1.266524e+18  North Carolina   United States  North Carolina  \n",
       "1.258013e+18       Boston, MA  United States   Massachusetts  \n",
       "1.261418e+18    Colorado, USA  United States        Colorado  \n",
       "1.256049e+18         New York  United States        New York  \n",
       "\n",
       "[166481 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "California              19014\n",
       "New York                17604\n",
       "Texas                   10918\n",
       "District of Columbia     8671\n",
       "Florida                  7484\n",
       "                        ...  \n",
       "52577                       1\n",
       "37064                       1\n",
       "37172                       1\n",
       "93238                       1\n",
       "60607                       1\n",
       "Name: state, Length: 6190, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa_tweets['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geonamescache import GeonamesCache\n",
    "\n",
    "gc = GeonamesCache()\n",
    "\n",
    "# Get list of known locations\n",
    "countries = gc.get_countries_by_names().keys()\n",
    "cities = [city['name'] for city in gc.get_cities().values()]\n",
    "\n",
    "# Create a set for fast lookup\n",
    "known_locations = set(cities + list(countries))\n",
    "\n",
    "def filter_location(location):\n",
    "    if pd.isnull(location):\n",
    "        return location\n",
    "    location_parts = location.split()\n",
    "    filtered_location = \" \".join([part for part in location_parts if part in known_locations])\n",
    "    return filtered_location if filtered_location else None\n",
    "\n",
    "tweets['UserLocation'] = tweets['UserLocation'].apply(filter_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t57¬∞35‚Ä≤46.6‚Ä≥N 13¬∞41‚Ä≤14.3‚Ä≥W' '\\n'\n",
      " '\\n\\nthese alters have developed into\\nhighly sophisticated and deadly\\naccurate instruments of tñ¶πtal\\ndestruction.\\n \\u200e·Ö† \\u200e \\n\\n‚Ä£ EST. JAN 2019\\n‚Ä£ MUN IS 29'\n",
      " ... '\\U0001fae7' '\\U0001fae7\\n‚ô°‚ÉõìçØ ' '\\U0001faf6üèæ lovers lane.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.unique(tweets['UserLocation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentiment(data):\n",
    "    # Split the 'Sentiment' column into two separate columns\n",
    "    data[['Positive_Sentiment', 'Negative_Sentiment']] = data['Sentiment'].str.split(' ', expand=True)\n",
    "\n",
    "    # Convert the sentiment columns to integers\n",
    "    data['Positive_Sentiment'] = data['Positive_Sentiment'].astype(int)\n",
    "    data['Negative_Sentiment'] = data['Negative_Sentiment'].astype(int)\n",
    "\n",
    "    # Calculate the average sentiment and create a new column\n",
    "    data['Average_Sentiment'] = (data['Positive_Sentiment'] + data['Negative_Sentiment']) / 2\n",
    "\n",
    "    # Create a new column 'Sentiment_Class' based on the 'Average_Sentiment' column\n",
    "    data['Sentiment_Class'] = data['Average_Sentiment'].apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\n",
    "\n",
    "    # Drop the original 'Sentiment' column\n",
    "    data.drop(columns=['Sentiment'], inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_or_process_data(input_folder, filename):\n",
    "    if input_folder is None:\n",
    "        input_folder = 'Data'\n",
    "        \n",
    "    if not os.path.exists(input_folder):\n",
    "        os.makedirs(input_folder)\n",
    "        \n",
    "    processed_filename = 'processed_' + os.path.splitext(filename)[0] + '.feather'\n",
    "    input_file_path = os.path.join(input_folder, filename)\n",
    "    output_file_path = os.path.join(input_folder, processed_filename)\n",
    "    \n",
    "    # Check if the processed file exists in the input folder\n",
    "    if os.path.exists(output_file_path):\n",
    "        # If it does, load and return the processed data\n",
    "        data = pd.read_feather(output_file_path)\n",
    "    else:\n",
    "        # If it doesn't, load the raw data from the input folder\n",
    "        data = pd.read_csv(input_file_path, index_col=1)[['TweetText', 'Sentiment']]\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        # Preprocess the tweets\n",
    "        data['TweetText'] = data['TweetText'].apply(preprocess_tweet)\n",
    "        \n",
    "        # Process the data\n",
    "        data = split_sentiment(data)\n",
    "        \n",
    "        # Save the processed data in the input folder\n",
    "        data.reset_index().drop(columns=['TweetId']).to_feather(output_file_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "input_folder = '/Users/antanas/GitRepo/SentPred/Data'\n",
    "filename = 'TweetsCOV19.csv'\n",
    "\n",
    "t_tweets = load_or_process_data(input_folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "def count_ngrams(tweets, n):\n",
    "    ngram_counts = Counter()\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet  # Update here\n",
    "        ngrams = get_ngrams(tokens, n)\n",
    "        ngram_counts.update(ngrams)\n",
    "    return ngram_counts\n",
    "\n",
    "def plot_most_common_ngrams_by_sentiment(tweets_df, ngram_range=(1, 1), num_ngrams=10, output_path=None):\n",
    "    sentiment_classes = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}\n",
    "\n",
    "    # Count n-grams by sentiment class\n",
    "    ngram_counts_by_sentiment = {}\n",
    "    for sentiment_class, sentiment_label in sentiment_classes.items():\n",
    "        ngram_counts = Counter()\n",
    "        sentiment_tweets = tweets_df[tweets_df['Sentiment_Class'] == sentiment_class]['TweetText']\n",
    "\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_counts.update(count_ngrams(sentiment_tweets, n))\n",
    "        \n",
    "        ngram_counts_by_sentiment[sentiment_label] = ngram_counts.most_common(num_ngrams)\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    ngram_count_data = []\n",
    "    for sentiment_label, ngram_counts in ngram_counts_by_sentiment.items():\n",
    "        for ngram, count in ngram_counts:\n",
    "            ngram_count_data.append({'N-gram': ' '.join(ngram), 'Count': count, 'Sentiment': sentiment_label})\n",
    "    \n",
    "    ngram_count_df = pd.DataFrame(ngram_count_data)\n",
    "\n",
    "    # Pivot DataFrame\n",
    "    ngram_count_pivot = ngram_count_df.pivot_table(values='Count', index='N-gram', columns='Sentiment', fill_value=0)\n",
    "\n",
    "    # Calculate total frequency and sort by it in descending order\n",
    "    ngram_count_pivot['Total_Frequency'] = ngram_count_pivot.sum(axis=1)\n",
    "    ngram_count_pivot.sort_values(by='Total_Frequency', ascending=False, inplace=True)\n",
    "    ngram_count_pivot.drop(columns=['Total_Frequency'], inplace=True)\n",
    "\n",
    "    # Plot stacked bar chart\n",
    "    plt.figure(figsize=(800, 400))\n",
    "    ngram_count_pivot.head(num_ngrams).plot(kind='bar', stacked=True, figsize=(16, 8))\n",
    "    plt.title(f'Most Common {ngram_range[0]}-{ngram_range[1]}-grams by Sentiment')\n",
    "    plt.xlabel('N-gram')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_most_common_ngrams_by_sentiment(t_tweets, ngram_range=(1, 1), num_ngrams=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "def plot_word_cloud(tweets_df, sentiment_label, output_path=None):\n",
    "    # Filter tweets by sentiment_label\n",
    "    sentiment_tweets = tweets_df[tweets_df['Sentiment_Class'] == sentiment_label]['TweetText']\n",
    "    \n",
    "    # Concatenate tweet texts\n",
    "    text = ' '.join([' '.join(tweet) for tweet in sentiment_tweets])\n",
    "\n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color='black', min_font_size=10).generate(text)\n",
    "\n",
    "    # Plot word cloud\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_word_cloud(t_tweets, sentiment_label=-1)  # 1 for positive, 0 for neutral, -1 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_sentiment_word_clouds(tweets_df):\n",
    "    sentiment_labels = [1, 0, -1]\n",
    "    sentiment_titles = ['Positive Sentiments', 'Neutral Sentiments', 'Negative Sentiments']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 8))\n",
    "    fig.suptitle('Word Clouds for Different Sentiment Labels', fontsize=20)\n",
    "\n",
    "    for i, sentiment_label in enumerate(sentiment_labels):\n",
    "        sentiment_tweets = tweets_df[tweets_df['Sentiment_Class'] == sentiment_label]['TweetText']\n",
    "        text = ' '.join([' '.join(tweet) for tweet in sentiment_tweets])\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=800, background_color='black', min_font_size=10).generate(text)\n",
    "\n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(sentiment_titles[i], fontsize=16)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_all_sentiment_word_clouds(t_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_sentiment_counts(tweets_df, output_path=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='Sentiment_Class', data=tweets_df, palette='coolwarm')\n",
    "    plt.title('Number of Positive, Negative, and Neutral Sentiments', fontsize=16)\n",
    "    plt.xlabel('Sentiment Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_sentiment_counts(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare splitting function for pd dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataframe(df, train_size=0.75, val_size=0.05, test_size=0.2, random_state=None):\n",
    "\n",
    "    # Split the DataFrame into training and test sets\n",
    "    train_df, test_df = train_test_split(df, train_size=train_size+val_size, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Split the training set into training and validation sets\n",
    "    train_df, val_df = train_test_split(train_df, train_size=train_size/(train_size+val_size), test_size=val_size/(train_size+val_size), random_state=random_state)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train, val, test = split_dataframe(t_tweets, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def evaluate_sentiment(tokens):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    text = ' '.join(tokens)\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "def process_and_save_vader_sentiments(tweets, chunk_size, output_folder, output_filename):\n",
    "    num_chunks = len(tweets) // chunk_size\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    output_file_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "    for i in range(num_chunks + 1):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        \n",
    "        chunk = tweets.iloc[start_idx:end_idx]\n",
    "        VADER_sentiments = chunk['TweetText'].apply(evaluate_sentiment)\n",
    "        \n",
    "        if os.path.exists(output_file_path) and i > 0:\n",
    "            mode = 'a'\n",
    "        else:\n",
    "            mode = 'w'\n",
    "        \n",
    "        VADER_sentiments.reset_index().to_feather(output_file_path, mode=mode)\n",
    "        print(f\"Processed and saved chunk {i + 1} of {num_chunks + 1}\")\n",
    "\n",
    "# Example usage\n",
    "chunk_size = 1000\n",
    "output_folder = 'Data/VADER_sentiments'\n",
    "output_filename = 'VADER_sentiments.feather'\n",
    "\n",
    "process_and_save_vader_sentiments(tweets, chunk_size, output_folder, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def evaluate_sentiment(tokens):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    text = ' '.join(tokens)\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "VADER_sentiments = tweets['TweetText'].apply(evaluate_sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your dataset\n",
    "# tweets_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# If you have already preprocessed the tweets and stored them in 't_tweets' DataFrame, you can use the following line:\n",
    "tweets = t_tweets['TweetText']\n",
    "\n",
    "# Join tokens into a single string for each tweet\n",
    "tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit the vectorizer to the tweets and transform the tweets into TF-IDF embeddings\n",
    "tfidf_embeddings = vectorizer.fit_transform(tweets_text)\n",
    "\n",
    "# Now, `tfidf_embeddings` is a sparse matrix containing the TF-IDF embeddings of the TweetsCOV19 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reduce dimensionality using TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "reduced_embeddings = svd.fit_transform(tfidf_embeddings)\n",
    "\n",
    "# Assuming 'sentiment_labels' contains the sentiment labels for each tweet in the dataset\n",
    "sentiment_labels = t_tweets['Sentiment_Class']\n",
    "\n",
    "# Create a scatter plot of the reduced embeddings with colors based on sentiment labels\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=2, c=sentiment_labels, cmap='jet')\n",
    "plt.title(\"TruncatedSVD projection of the TweetsCOV19 dataset\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set()\n",
    "\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    for token in tweet:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "vocabulary_size = len(unique_tokens)\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load your dataset\n",
    "# tweets_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# If you have already preprocessed the tweets and stored them in 't_tweets' DataFrame, you can use the following line:\n",
    "tweets = t_tweets['TweetText']\n",
    "\n",
    "# Join tokens into a single string for each tweet\n",
    "tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit the vectorizer to the tweets and transform the tweets into Bag of Words embeddings\n",
    "bow_embeddings = vectorizer.fit_transform(tweets_text)\n",
    "\n",
    "# Apply TruncatedSVD to reduce dimensionality\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "reduced_embeddings = svd.fit_transform(bow_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'sentiment_labels' contains the sentiment labels for each tweet in the dataset\n",
    "sentiment_labels = t_tweets['Sentiment_Class']\n",
    "# Create a scatter plot of the reduced embeddings with colors based on sentiment labels\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=2, c=sentiment_labels, cmap='jet')\n",
    "plt.title(\"TruncatedSVD projection of the TweetsCOV19 dataset\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Create an embedding matrix for the dataset\n",
    "def create_embedding_matrix(word_index, embeddings, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in embeddings:\n",
    "            embedding_vector = embeddings[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Create a word index for the TweetsCOV19 dataset\n",
    "word_index = {}\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    tokens = tweet\n",
    "    for token in tokens:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = len(word_index) + 1\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix(word_index, model, embedding_dim)\n",
    "\n",
    "# Generate embeddings for each tweet\n",
    "tweet_embeddings = []\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    tokens = tweet\n",
    "    token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "    tweet_embedding = np.mean([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Now, `tweet_embeddings` contains the Word2Vec embeddings for each tweet in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def Word2Vec_embed(tweets, aggregation_method='mean'):\n",
    "    # Load pre-trained Word2Vec embeddings\n",
    "    model = api.load(\"word2vec-google-news-300\")\n",
    "    embedding_dim = 300\n",
    "\n",
    "    # If aggregation method is weighted_average, calculate TF-IDF scores\n",
    "    if aggregation_method == 'weighted_average':\n",
    "        tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        tfidf_scores = vectorizer.fit_transform(tweets_text)\n",
    "        tfidf_dict = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "    # Generate embeddings for each tweet\n",
    "    tweet_embeddings = []\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet\n",
    "        token_vectors = [model[token] for token in tokens if token in model]\n",
    "\n",
    "        if aggregation_method == 'weighted_average':\n",
    "            token_weights = [tfidf_scores[idx, tfidf_dict[token]] for idx, token in enumerate(tokens) if token in model and token in tfidf_dict]\n",
    "            if len(token_vectors) > 0:\n",
    "                tweet_embedding = np.average(token_vectors, axis=0, weights=token_weights)\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        else:\n",
    "            if len(token_vectors) > 0:\n",
    "                if aggregation_method == 'mean':\n",
    "                    tweet_embedding = np.mean(token_vectors, axis=0)\n",
    "                elif aggregation_method == 'sum':\n",
    "                    tweet_embedding = np.sum(token_vectors, axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid aggregation method\")\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "\n",
    "        tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "    return np.array(tweet_embeddings)\n",
    "\n",
    "# Example usage:\n",
    "tweet_embeddings = Word2Vec_embed(t_tweets['TweetText'], aggregation_method='mean')\n",
    "# You can change 'mean' to 'sum' or 'weighted_average' as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import umap_ as umap\n",
    "\n",
    "def visualize_embeddings(embeddings, sentiment_labels, method='pca', random_state=42, sample_size=None):\n",
    "    if sample_size is not None and sample_size < len(embeddings):\n",
    "        sentiment_labels = np.array(sentiment_labels)\n",
    "        unique_labels = np.unique(sentiment_labels)\n",
    "        sample_size_per_label = sample_size // len(unique_labels)\n",
    "        idx = []\n",
    "\n",
    "        for label in unique_labels:\n",
    "            label_idx = np.where(sentiment_labels == label)[0]\n",
    "            label_sample_idx = np.random.choice(label_idx, sample_size_per_label, replace=False)\n",
    "            idx.extend(label_sample_idx)\n",
    "\n",
    "        np.random.shuffle(idx)\n",
    "        embeddings = embeddings[idx]\n",
    "        sentiment_labels = sentiment_labels[idx]\n",
    "\n",
    "    if method.lower() == 'pca':\n",
    "        reducer = PCA(n_components=2, random_state=random_state)\n",
    "    elif method.lower() == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=random_state)\n",
    "    elif method.lower() == 'umap':\n",
    "        reducer = umap.UMAP(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'pca', 'tsne', or 'umap'\")\n",
    "\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=2, c=sentiment_labels, cmap='viridis', alpha=0.8)\n",
    "    plt.title(f\"{method.upper()} Visualization of Tweet Embeddings\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    \n",
    "    # Add a colorbar to show the sentiment labels\n",
    "    cbar = plt.colorbar(scatter, ticks=[-1, 0, 1])\n",
    "    cbar.ax.set_yticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `sentiment_labels` is an array of sentiment labels (0 for negative, 1 for neutral, and 2 for positive) for each tweet\n",
    "visualize_embeddings(tweet_embeddings, sentiment_labels = t_tweets['Sentiment_Class'], method='umap', sample_size=1000)\n",
    "# You can change 'pca' to 'tsne' or 'umap' as needed\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Create an embedding matrix for the dataset\n",
    "def create_embedding_matrix(word_index, embeddings, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Download the GloVe embeddings if you haven't already\n",
    "# You can download them from https://nlp.stanford.edu/projects/glove/\n",
    "# For this example, we will use the 100-dimensional GloVe embeddings\n",
    "glove_file = '/Users/antanas/GitRepo/SentPred/models/glove/glove.6B.50d.txt'\n",
    "\n",
    "# Load the pre-trained GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "# Create a word index for the TweetsCOV19 dataset\n",
    "word_index = {}\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    for token in tweet:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = len(word_index) + 1\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, embedding_dim)\n",
    "\n",
    "# Create an embedding layer using the embedding matrix\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "\n",
    "# Use the embedding layer to generate embeddings for each tweet\n",
    "tweet_embeddings = []\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    token_indices = [word_index[token] for token in tweet if token in word_index]\n",
    "    token_indices_tensor = torch.tensor(token_indices, dtype=torch.long)\n",
    "    tweet_embedding = embedding_layer(token_indices_tensor)\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Now, `tweet_embeddings` contains the GloVe embeddings for each tweet in the dataset\n",
    "tweet_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def GloVe_embed(tweets, glove_file, aggregation_method='mean'):\n",
    "    # Load pre-trained GloVe embeddings\n",
    "    glove_embeddings = load_glove_embeddings(glove_file)\n",
    "    embedding_dim = 50\n",
    "\n",
    "    # Create a word index for the dataset\n",
    "    word_index = {}\n",
    "    for tweet in tweets:\n",
    "        for token in tweet:\n",
    "            if token not in word_index:\n",
    "                word_index[token] = len(word_index) + 1\n",
    "\n",
    "    # Create the embedding matrix\n",
    "    embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, embedding_dim)\n",
    "\n",
    "    # If aggregation method is weighted_average, calculate TF-IDF scores\n",
    "    if aggregation_method == 'weighted_average':\n",
    "        tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        tfidf_scores = vectorizer.fit_transform(tweets_text)\n",
    "        tfidf_dict = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "    # Generate embeddings for each tweet\n",
    "    tweet_embeddings = []\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet\n",
    "        if aggregation_method == 'weighted_average':\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            token_weights = [tfidf_scores[word_index[token], tfidf_dict[token]] for token in tokens if token in word_index and token in tfidf_dict]\n",
    "            if len(token_indices) > 0:\n",
    "                tweet_embedding = np.average([embedding_matrix[idx] for idx in token_indices], axis=0, weights=token_weights)\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        else:\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            if len(token_indices) > 0:\n",
    "                if aggregation_method == 'mean':\n",
    "                    tweet_embedding = np.mean([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                elif aggregation_method == 'sum':\n",
    "                    tweet_embedding = np.sum([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid aggregation method\")\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "    return np.array(tweet_embeddings)\n",
    "\n",
    "# Example usage:\n",
    "glove_file = '/path/to/glove.6B.50d.txt'\n",
    "tweet_embeddings = GloVe_embed(t_tweets['TweetText'], glove_file, aggregation_method='mean')\n",
    "# You can change 'mean' to 'sum' or 'weighted_average' as needed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained FastText embeddings\n",
    "def load_fasttext_embeddings(fasttext_file):\n",
    "    return KeyedVectors.load_word2vec_format(fasttext_file)\n",
    "\n",
    "# Create an embedding matrix for the dataset\n",
    "def create_embedding_matrix(word_index, embeddings, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in embeddings:\n",
    "            embedding_vector = embeddings[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Download the FastText embeddings if you haven't already\n",
    "fasttext_file = '/Users/antanas/GitRepo/SentPred/models/FastText/wiki-news-300d-1M.vec'\n",
    "\n",
    "# Load the pre-trained FastText embeddings\n",
    "fasttext_embeddings = load_fasttext_embeddings(fasttext_file)\n",
    "\n",
    "# Create a word index for the TweetsCOV19 dataset\n",
    "word_index = {}\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    for token in tweet:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = len(word_index) + 1\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix(word_index, fasttext_embeddings, embedding_dim)\n",
    "\n",
    "# Create an embedding layer using the embedding matrix\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "\n",
    "# Use the embedding layer to generate embeddings for each tweet\n",
    "tweet_embeddings = []\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    token_indices = [word_index[token] for token in tweet if token in word_index]\n",
    "    token_indices_tensor = torch.tensor(token_indices, dtype=torch.long)\n",
    "    tweet_embedding = embedding_layer(token_indices_tensor)\n",
    "    \n",
    "    # Average the embeddings of all words in the tweet\n",
    "    avg_embedding = torch.mean(tweet_embedding, dim=0)\n",
    "    tweet_embeddings.append(avg_embedding)\n",
    "\n",
    "# Now, `tweet_embeddings` contains the average FastText embeddings for each tweet in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def FastText_embed(tweets, fasttext_file, aggregation_method='mean'):\n",
    "    # Load pre-trained FastText embeddings\n",
    "    fasttext_embeddings = load_fasttext_embeddings(fasttext_file)\n",
    "    embedding_dim = 300\n",
    "\n",
    "    # Create a word index for the dataset\n",
    "    word_index = {}\n",
    "    for tweet in tweets:\n",
    "        for token in tweet:\n",
    "            if token not in word_index:\n",
    "                word_index[token] = len(word_index) + 1\n",
    "\n",
    "    # Create the embedding matrix\n",
    "    embedding_matrix = create_embedding_matrix(word_index, fasttext_embeddings, embedding_dim)\n",
    "\n",
    "    # If aggregation method is weighted_average, calculate TF-IDF scores\n",
    "    if aggregation_method == 'weighted_average':\n",
    "        tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        tfidf_scores = vectorizer.fit_transform(tweets_text)\n",
    "        tfidf_dict = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "    # Generate embeddings for each tweet\n",
    "    tweet_embeddings = []\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet\n",
    "        if aggregation_method == 'weighted_average':\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            token_weights = [tfidf_scores[word_index[token], tfidf_dict[token]] for token in tokens if token in word_index and token in tfidf_dict]\n",
    "            if len(token_indices) > 0:\n",
    "                tweet_embedding = np.average([embedding_matrix[idx] for idx in token_indices], axis=0, weights=token_weights)\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        else:\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            if len(token_indices) > 0:\n",
    "                if aggregation_method == 'mean':\n",
    "                    tweet_embedding = np.mean([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                elif aggregation_method == 'sum':\n",
    "                    tweet_embedding = np.sum([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid aggregation method\")\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "    return np.array(tweet_embeddings)\n",
    "\n",
    "# Example usage:\n",
    "fasttext_file = '/Users/antanas/GitRepo/SentPred/models/FastText/wiki-news-300d-1M.vec'\n",
    "tweet_embeddings = FastText_embed(t_tweets['TweetText'], fasttext_file, aggregation_method='mean')\n",
    "# You can change 'mean' to 'sum' or 'weighted_average' as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
