{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'tweet', 'hashtags', 'emojis']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Add these lines at the beginning of your code\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "abbreviations = {\n",
    "    'u': 'you',\n",
    "    'r': 'are',\n",
    "    'lol': 'laugh out loud',\n",
    "    'omg': 'oh my god',\n",
    "    'np': 'no problem',\n",
    "    'wtf': 'what the fuck',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'lmfao': 'laughing my fucking ass off',\n",
    "    'idk': 'I don\\'t know',\n",
    "    'btw': 'by the way',\n",
    "    'jk': 'just kidding',\n",
    "    'tbh': 'to be honest',\n",
    "    'fyi': 'for your information',\n",
    "    'imo': 'in my opinion',\n",
    "    'imho': 'in my humble opinion',\n",
    "    'nvm': 'never mind',\n",
    "    'thx': 'thanks',\n",
    "    'tks': 'thanks',\n",
    "    'plz': 'please',\n",
    "    'w/': 'with',\n",
    "    'w/o': 'without',\n",
    "    'b/c': 'because',\n",
    "    'wbu': 'what about you',\n",
    "    'w/e': 'whatever',\n",
    "    'gr8': 'great',\n",
    "    'tho': 'though',\n",
    "    'ppl': 'people',\n",
    "    'fml': 'fuck my life',\n",
    "    'nbd': 'no big deal',\n",
    "    'bff': 'best friends forever',\n",
    "    'ikr': 'I know, right?',\n",
    "    'smh': 'shaking my head',\n",
    "    'irl': 'in real life',\n",
    "    'ik': 'I know',\n",
    "    'sry': 'sorry',\n",
    "    'thn': 'then',\n",
    "    'k': 'okay',\n",
    "    'wbu': 'what about you',\n",
    "    'ic': 'I see',\n",
    "    'afaik': 'as far as I know',\n",
    "    'brb': 'be right back',\n",
    "    'g2g': 'got to go',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'hmu': 'hit me up',\n",
    "    'fomo': 'fear of missing out',\n",
    "    'imy': 'I miss you',\n",
    "    'tb': 'text back',\n",
    "    'g1': 'good one',\n",
    "    'w8': 'wait',\n",
    "    'meh': 'whatever',\n",
    "    'rly': 'really',\n",
    "    'cya': 'see ya',\n",
    "    'gtg': 'got to go',\n",
    "    'ttys': 'talk to you soon',\n",
    "    'l8r': 'later',\n",
    "    'nm': 'nothing much',\n",
    "    'cu': 'see you',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'b4': 'before',\n",
    "    'afk': 'away from keyboard',\n",
    "    'tmi': 'too much information',\n",
    "    'ty': 'thank you',\n",
    "    'njoy': 'enjoy',\n",
    "    'tho': 'though',\n",
    "    'f2f': 'face to face',\n",
    "    'idc': 'I don\\'t care',\n",
    "    'fyi': 'for your information',\n",
    "    'wth': 'what the hell',\n",
    "    'yolo': 'you only live once',\n",
    "    'gratz': 'congratulations',\n",
    "    'gr8': 'great',\n",
    "    'ily': 'I love you',\n",
    "    'thks': 'thanks',\n",
    "    'luv': 'love',\n",
    "}\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # 3. Removing mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # # 4. Removing hashtags\n",
    "    # text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # 4. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 5. Removing emojis\n",
    "    tokens = [token for token in tokens if not any(c in emoji.EMOJI_DATA for c in token)]\n",
    "\n",
    "    # 6. Removing special characters and punctuation\n",
    "    tokens = [re.sub(r'\\W+|\\d+', '', token) for token in tokens]\n",
    "\n",
    "    # 7. Expanding contractions and abbreviations using contractions package\n",
    "    tokens = [contractions.fix(token) for token in tokens]\n",
    "\n",
    "    # 8. Replace abbreviations\n",
    "    tokens = [abbreviations.get(token, token) for token in tokens]\n",
    "\n",
    "    # 9. Removing stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # 10. Lemmatization or stemming\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokens = [token for token in tokens if token != '']\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tweet = \"Here's an example, tweet with @mentions, http://urls.com, #hashtags and ðŸ˜ƒ emojis!\"\n",
    "preprocessed_tweet = preprocess_tweet(tweet)\n",
    "print(preprocessed_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# tweets = pd.read_csv('/Users/antanas/GitRepo/SentPred/Data/TweetsCOV19.csv', index_col=0, dtype={'TweetId': str, 'Username': str,\n",
    "#                                                                                                  'Timestamp': str, 'NoFollowers': int,\n",
    "#                                                                                                  'NoFriends': int, 'NoRetweets': int,\n",
    "#                                                                                                  'NoFavorites': int, 'Entities': str,\n",
    "#                                                                                                  'Sentiment': str, 'Mentions': str,\n",
    "#                                                                                                  'Hashtags': str, 'URLs': str,\n",
    "#                                                                                                  'TweetText': str, 'UserLocation': str})\n",
    "tweets = pd.read_csv('/Users/antanas/GitRepo/SentPred/Data/TweetsCOV19.csv', index_col=1)[['TweetText', 'Sentiment']]\n",
    "tweets.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentiment(data):\n",
    "    # Split the 'Sentiment' column into two separate columns\n",
    "    data[['Positive_Sentiment', 'Negative_Sentiment']] = data['Sentiment'].str.split(' ', expand=True)\n",
    "\n",
    "    # Convert the sentiment columns to integers\n",
    "    data['Positive_Sentiment'] = data['Positive_Sentiment'].astype(int)\n",
    "    data['Negative_Sentiment'] = data['Negative_Sentiment'].astype(int)\n",
    "\n",
    "    # Calculate the average sentiment and create a new column\n",
    "    data['Average_Sentiment'] = (data['Positive_Sentiment'] + data['Negative_Sentiment']) / 2\n",
    "\n",
    "    # Create a new column 'Sentiment_Class' based on the 'Average_Sentiment' column\n",
    "    data['Sentiment_Class'] = data['Average_Sentiment'].apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\n",
    "\n",
    "    # Drop the original 'Sentiment' column\n",
    "    data.drop(columns=['Sentiment'], inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_or_process_data(input_folder, filename):\n",
    "    if input_folder is None:\n",
    "        input_folder = 'Data'\n",
    "        \n",
    "    if not os.path.exists(input_folder):\n",
    "        os.makedirs(input_folder)\n",
    "        \n",
    "    processed_filename = 'processed_' + os.path.splitext(filename)[0] + '.feather'\n",
    "    input_file_path = os.path.join(input_folder, filename)\n",
    "    output_file_path = os.path.join(input_folder, processed_filename)\n",
    "    \n",
    "    # Check if the processed file exists in the input folder\n",
    "    if os.path.exists(output_file_path):\n",
    "        # If it does, load and return the processed data\n",
    "        data = pd.read_feather(output_file_path)\n",
    "    else:\n",
    "        # If it doesn't, load the raw data from the input folder\n",
    "        data = pd.read_csv(input_file_path, index_col=1)[['TweetText', 'Sentiment']]\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        # Preprocess the tweets\n",
    "        data['TweetText'] = data['TweetText'].apply(preprocess_tweet)\n",
    "        \n",
    "        # Process the data\n",
    "        data = split_sentiment(data)\n",
    "        \n",
    "        # Save the processed data in the input folder\n",
    "        data.reset_index().drop(columns=['TweetId']).to_feather(output_file_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "input_folder = '/Users/antanas/GitRepo/SentPred/Data'\n",
    "filename = 'TweetsCOV19.csv'\n",
    "\n",
    "t_tweets = load_or_process_data(input_folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "def count_ngrams(tweets, n):\n",
    "    ngram_counts = Counter()\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet  # Update here\n",
    "        ngrams = get_ngrams(tokens, n)\n",
    "        ngram_counts.update(ngrams)\n",
    "    return ngram_counts\n",
    "\n",
    "def plot_most_common_ngrams_by_sentiment(tweets_df, ngram_range=(1, 1), num_ngrams=10, output_path=None):\n",
    "    sentiment_classes = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}\n",
    "\n",
    "    # Count n-grams by sentiment class\n",
    "    ngram_counts_by_sentiment = {}\n",
    "    for sentiment_class, sentiment_label in sentiment_classes.items():\n",
    "        ngram_counts = Counter()\n",
    "        sentiment_tweets = tweets_df[tweets_df['Sentiment_Class'] == sentiment_class]['TweetText']\n",
    "\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_counts.update(count_ngrams(sentiment_tweets, n))\n",
    "        \n",
    "        ngram_counts_by_sentiment[sentiment_label] = ngram_counts.most_common(num_ngrams)\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    ngram_count_data = []\n",
    "    for sentiment_label, ngram_counts in ngram_counts_by_sentiment.items():\n",
    "        for ngram, count in ngram_counts:\n",
    "            ngram_count_data.append({'N-gram': ' '.join(ngram), 'Count': count, 'Sentiment': sentiment_label})\n",
    "    \n",
    "    ngram_count_df = pd.DataFrame(ngram_count_data)\n",
    "\n",
    "    # Pivot DataFrame\n",
    "    ngram_count_pivot = ngram_count_df.pivot_table(values='Count', index='N-gram', columns='Sentiment', fill_value=0)\n",
    "\n",
    "    # Calculate total frequency and sort by it in descending order\n",
    "    ngram_count_pivot['Total_Frequency'] = ngram_count_pivot.sum(axis=1)\n",
    "    ngram_count_pivot.sort_values(by='Total_Frequency', ascending=False, inplace=True)\n",
    "    ngram_count_pivot.drop(columns=['Total_Frequency'], inplace=True)\n",
    "\n",
    "    # Plot stacked bar chart\n",
    "    plt.figure(figsize=(800, 400))\n",
    "    ngram_count_pivot.head(num_ngrams).plot(kind='bar', stacked=True, figsize=(16, 8))\n",
    "    plt.title(f'Most Common {ngram_range[0]}-{ngram_range[1]}-grams by Sentiment')\n",
    "    plt.xlabel('N-gram')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_most_common_ngrams_by_sentiment(t_tweets, ngram_range=(1, 1), num_ngrams=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "def plot_word_cloud(tweets_df, sentiment_label, output_path=None):\n",
    "    # Filter tweets by sentiment_label\n",
    "    sentiment_tweets = tweets_df[tweets_df['Sentiment_Class'] == sentiment_label]['TweetText']\n",
    "    \n",
    "    # Concatenate tweet texts\n",
    "    text = ' '.join([' '.join(tweet) for tweet in sentiment_tweets])\n",
    "\n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color='black', min_font_size=10).generate(text)\n",
    "\n",
    "    # Plot word cloud\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_word_cloud(t_tweets, sentiment_label=-1)  # 1 for positive, 0 for neutral, -1 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_sentiment_word_clouds(tweets_df):\n",
    "    sentiment_labels = [1, 0, -1]\n",
    "    sentiment_titles = ['Positive Sentiments', 'Neutral Sentiments', 'Negative Sentiments']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 8))\n",
    "    fig.suptitle('Word Clouds for Different Sentiment Labels', fontsize=20)\n",
    "\n",
    "    for i, sentiment_label in enumerate(sentiment_labels):\n",
    "        sentiment_tweets = tweets_df[tweets_df['Sentiment_Class'] == sentiment_label]['TweetText']\n",
    "        text = ' '.join([' '.join(tweet) for tweet in sentiment_tweets])\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=800, background_color='black', min_font_size=10).generate(text)\n",
    "\n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(sentiment_titles[i], fontsize=16)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_all_sentiment_word_clouds(t_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_sentiment_counts(tweets_df, output_path=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='Sentiment_Class', data=tweets_df, palette='coolwarm')\n",
    "    plt.title('Number of Positive, Negative, and Neutral Sentiments', fontsize=16)\n",
    "    plt.xlabel('Sentiment Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_sentiment_counts(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare splitting function for pd dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataframe(df, train_size=0.75, val_size=0.05, test_size=0.2, random_state=None):\n",
    "\n",
    "    # Split the DataFrame into training and test sets\n",
    "    train_df, test_df = train_test_split(df, train_size=train_size+val_size, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Split the training set into training and validation sets\n",
    "    train_df, val_df = train_test_split(train_df, train_size=train_size/(train_size+val_size), test_size=val_size/(train_size+val_size), random_state=random_state)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train, val, test = split_dataframe(t_tweets, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def evaluate_sentiment(tokens):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    text = ' '.join(tokens)\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "def process_and_save_vader_sentiments(tweets, chunk_size, output_folder, output_filename):\n",
    "    num_chunks = len(tweets) // chunk_size\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    output_file_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "    for i in range(num_chunks + 1):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        \n",
    "        chunk = tweets.iloc[start_idx:end_idx]\n",
    "        VADER_sentiments = chunk['TweetText'].apply(evaluate_sentiment)\n",
    "        \n",
    "        if os.path.exists(output_file_path) and i > 0:\n",
    "            mode = 'a'\n",
    "        else:\n",
    "            mode = 'w'\n",
    "        \n",
    "        VADER_sentiments.reset_index().to_feather(output_file_path, mode=mode)\n",
    "        print(f\"Processed and saved chunk {i + 1} of {num_chunks + 1}\")\n",
    "\n",
    "# Example usage\n",
    "chunk_size = 1000\n",
    "output_folder = 'Data/VADER_sentiments'\n",
    "output_filename = 'VADER_sentiments.feather'\n",
    "\n",
    "process_and_save_vader_sentiments(tweets, chunk_size, output_folder, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def evaluate_sentiment(tokens):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    text = ' '.join(tokens)\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "VADER_sentiments = tweets['TweetText'].apply(evaluate_sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your dataset\n",
    "# tweets_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# If you have already preprocessed the tweets and stored them in 't_tweets' DataFrame, you can use the following line:\n",
    "tweets = t_tweets['TweetText']\n",
    "\n",
    "# Join tokens into a single string for each tweet\n",
    "tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit the vectorizer to the tweets and transform the tweets into TF-IDF embeddings\n",
    "tfidf_embeddings = vectorizer.fit_transform(tweets_text)\n",
    "\n",
    "# Now, `tfidf_embeddings` is a sparse matrix containing the TF-IDF embeddings of the TweetsCOV19 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reduce dimensionality using TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "reduced_embeddings = svd.fit_transform(tfidf_embeddings)\n",
    "\n",
    "# Assuming 'sentiment_labels' contains the sentiment labels for each tweet in the dataset\n",
    "sentiment_labels = t_tweets['Sentiment_Class']\n",
    "\n",
    "# Create a scatter plot of the reduced embeddings with colors based on sentiment labels\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=2, c=sentiment_labels, cmap='jet')\n",
    "plt.title(\"TruncatedSVD projection of the TweetsCOV19 dataset\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set()\n",
    "\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    for token in tweet:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "vocabulary_size = len(unique_tokens)\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load your dataset\n",
    "# tweets_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# If you have already preprocessed the tweets and stored them in 't_tweets' DataFrame, you can use the following line:\n",
    "tweets = t_tweets['TweetText']\n",
    "\n",
    "# Join tokens into a single string for each tweet\n",
    "tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit the vectorizer to the tweets and transform the tweets into Bag of Words embeddings\n",
    "bow_embeddings = vectorizer.fit_transform(tweets_text)\n",
    "\n",
    "# Apply TruncatedSVD to reduce dimensionality\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "reduced_embeddings = svd.fit_transform(bow_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'sentiment_labels' contains the sentiment labels for each tweet in the dataset\n",
    "sentiment_labels = t_tweets['Sentiment_Class']\n",
    "# Create a scatter plot of the reduced embeddings with colors based on sentiment labels\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=2, c=sentiment_labels, cmap='jet')\n",
    "plt.title(\"TruncatedSVD projection of the TweetsCOV19 dataset\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Create an embedding matrix for the dataset\n",
    "def create_embedding_matrix(word_index, embeddings, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in embeddings:\n",
    "            embedding_vector = embeddings[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Create a word index for the TweetsCOV19 dataset\n",
    "word_index = {}\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    tokens = tweet\n",
    "    for token in tokens:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = len(word_index) + 1\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix(word_index, model, embedding_dim)\n",
    "\n",
    "# Generate embeddings for each tweet\n",
    "tweet_embeddings = []\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    tokens = tweet\n",
    "    token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "    tweet_embedding = np.mean([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Now, `tweet_embeddings` contains the Word2Vec embeddings for each tweet in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def Word2Vec_embed(tweets, aggregation_method='mean'):\n",
    "    # Load pre-trained Word2Vec embeddings\n",
    "    model = api.load(\"word2vec-google-news-300\")\n",
    "    embedding_dim = 300\n",
    "\n",
    "    # If aggregation method is weighted_average, calculate TF-IDF scores\n",
    "    if aggregation_method == 'weighted_average':\n",
    "        tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        tfidf_scores = vectorizer.fit_transform(tweets_text)\n",
    "        tfidf_dict = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "    # Generate embeddings for each tweet\n",
    "    tweet_embeddings = []\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet\n",
    "        token_vectors = [model[token] for token in tokens if token in model]\n",
    "\n",
    "        if aggregation_method == 'weighted_average':\n",
    "            token_weights = [tfidf_scores[idx, tfidf_dict[token]] for idx, token in enumerate(tokens) if token in model and token in tfidf_dict]\n",
    "            if len(token_vectors) > 0:\n",
    "                tweet_embedding = np.average(token_vectors, axis=0, weights=token_weights)\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        else:\n",
    "            if len(token_vectors) > 0:\n",
    "                if aggregation_method == 'mean':\n",
    "                    tweet_embedding = np.mean(token_vectors, axis=0)\n",
    "                elif aggregation_method == 'sum':\n",
    "                    tweet_embedding = np.sum(token_vectors, axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid aggregation method\")\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "\n",
    "        tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "    return np.array(tweet_embeddings)\n",
    "\n",
    "# Example usage:\n",
    "tweet_embeddings = Word2Vec_embed(t_tweets['TweetText'], aggregation_method='mean')\n",
    "# You can change 'mean' to 'sum' or 'weighted_average' as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import umap_ as umap\n",
    "\n",
    "def visualize_embeddings(embeddings, sentiment_labels, method='pca', random_state=42):\n",
    "    if method.lower() == 'pca':\n",
    "        reducer = PCA(n_components=2, random_state=random_state)\n",
    "    elif method.lower() == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=random_state)\n",
    "    elif method.lower() == 'umap':\n",
    "        reducer = umap.UMAP(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'pca', 'tsne', or 'umap'\")\n",
    "\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=2, c=sentiment_labels, cmap='viridis', alpha=0.8)\n",
    "    plt.title(f\"{method.upper()} Visualization of Tweet Embeddings\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    \n",
    "    # Add a colorbar to show the sentiment labels\n",
    "    cbar = plt.colorbar(scatter, ticks=[-1, 0, 1])\n",
    "    cbar.ax.set_yticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `sentiment_labels` is an array of sentiment labels (0 for negative, 1 for neutral, and 2 for positive) for each tweet\n",
    "visualize_embeddings(tweet_embeddings, sentiment_labels = t_tweets['Sentiment_Class'], method='tsne')\n",
    "# You can change 'pca' to 'tsne' or 'umap' as needed\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Create an embedding matrix for the dataset\n",
    "def create_embedding_matrix(word_index, embeddings, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Download the GloVe embeddings if you haven't already\n",
    "# You can download them from https://nlp.stanford.edu/projects/glove/\n",
    "# For this example, we will use the 100-dimensional GloVe embeddings\n",
    "glove_file = '/Users/antanas/GitRepo/SentPred/models/glove/glove.6B.50d.txt'\n",
    "\n",
    "# Load the pre-trained GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "# Create a word index for the TweetsCOV19 dataset\n",
    "word_index = {}\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    for token in tweet:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = len(word_index) + 1\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, embedding_dim)\n",
    "\n",
    "# Create an embedding layer using the embedding matrix\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "\n",
    "# Use the embedding layer to generate embeddings for each tweet\n",
    "tweet_embeddings = []\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    token_indices = [word_index[token] for token in tweet if token in word_index]\n",
    "    token_indices_tensor = torch.tensor(token_indices, dtype=torch.long)\n",
    "    tweet_embedding = embedding_layer(token_indices_tensor)\n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Now, `tweet_embeddings` contains the GloVe embeddings for each tweet in the dataset\n",
    "tweet_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def GloVe_embed(tweets, glove_file, aggregation_method='mean'):\n",
    "    # Load pre-trained GloVe embeddings\n",
    "    glove_embeddings = load_glove_embeddings(glove_file)\n",
    "    embedding_dim = 50\n",
    "\n",
    "    # Create a word index for the dataset\n",
    "    word_index = {}\n",
    "    for tweet in tweets:\n",
    "        for token in tweet:\n",
    "            if token not in word_index:\n",
    "                word_index[token] = len(word_index) + 1\n",
    "\n",
    "    # Create the embedding matrix\n",
    "    embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, embedding_dim)\n",
    "\n",
    "    # If aggregation method is weighted_average, calculate TF-IDF scores\n",
    "    if aggregation_method == 'weighted_average':\n",
    "        tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        tfidf_scores = vectorizer.fit_transform(tweets_text)\n",
    "        tfidf_dict = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "    # Generate embeddings for each tweet\n",
    "    tweet_embeddings = []\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet\n",
    "        if aggregation_method == 'weighted_average':\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            token_weights = [tfidf_scores[word_index[token], tfidf_dict[token]] for token in tokens if token in word_index and token in tfidf_dict]\n",
    "            if len(token_indices) > 0:\n",
    "                tweet_embedding = np.average([embedding_matrix[idx] for idx in token_indices], axis=0, weights=token_weights)\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        else:\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            if len(token_indices) > 0:\n",
    "                if aggregation_method == 'mean':\n",
    "                    tweet_embedding = np.mean([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                elif aggregation_method == 'sum':\n",
    "                    tweet_embedding = np.sum([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid aggregation method\")\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "    return np.array(tweet_embeddings)\n",
    "\n",
    "# Example usage:\n",
    "glove_file = '/path/to/glove.6B.50d.txt'\n",
    "tweet_embeddings = GloVe_embed(t_tweets['TweetText'], glove_file, aggregation_method='mean')\n",
    "# You can change 'mean' to 'sum' or 'weighted_average' as needed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained FastText embeddings\n",
    "def load_fasttext_embeddings(fasttext_file):\n",
    "    return KeyedVectors.load_word2vec_format(fasttext_file)\n",
    "\n",
    "# Create an embedding matrix for the dataset\n",
    "def create_embedding_matrix(word_index, embeddings, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in embeddings:\n",
    "            embedding_vector = embeddings[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Download the FastText embeddings if you haven't already\n",
    "fasttext_file = '/Users/antanas/GitRepo/SentPred/models/FastText/wiki-news-300d-1M.vec'\n",
    "\n",
    "# Load the pre-trained FastText embeddings\n",
    "fasttext_embeddings = load_fasttext_embeddings(fasttext_file)\n",
    "\n",
    "# Create a word index for the TweetsCOV19 dataset\n",
    "word_index = {}\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    for token in tweet:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = len(word_index) + 1\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix(word_index, fasttext_embeddings, embedding_dim)\n",
    "\n",
    "# Create an embedding layer using the embedding matrix\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "\n",
    "# Use the embedding layer to generate embeddings for each tweet\n",
    "tweet_embeddings = []\n",
    "for tweet in t_tweets['TweetText']:\n",
    "    token_indices = [word_index[token] for token in tweet if token in word_index]\n",
    "    token_indices_tensor = torch.tensor(token_indices, dtype=torch.long)\n",
    "    tweet_embedding = embedding_layer(token_indices_tensor)\n",
    "    \n",
    "    # Average the embeddings of all words in the tweet\n",
    "    avg_embedding = torch.mean(tweet_embedding, dim=0)\n",
    "    tweet_embeddings.append(avg_embedding)\n",
    "\n",
    "# Now, `tweet_embeddings` contains the average FastText embeddings for each tweet in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def FastText_embed(tweets, fasttext_file, aggregation_method='mean'):\n",
    "    # Load pre-trained FastText embeddings\n",
    "    fasttext_embeddings = load_fasttext_embeddings(fasttext_file)\n",
    "    embedding_dim = 300\n",
    "\n",
    "    # Create a word index for the dataset\n",
    "    word_index = {}\n",
    "    for tweet in tweets:\n",
    "        for token in tweet:\n",
    "            if token not in word_index:\n",
    "                word_index[token] = len(word_index) + 1\n",
    "\n",
    "    # Create the embedding matrix\n",
    "    embedding_matrix = create_embedding_matrix(word_index, fasttext_embeddings, embedding_dim)\n",
    "\n",
    "    # If aggregation method is weighted_average, calculate TF-IDF scores\n",
    "    if aggregation_method == 'weighted_average':\n",
    "        tweets_text = [' '.join(tweet) for tweet in tweets]\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        tfidf_scores = vectorizer.fit_transform(tweets_text)\n",
    "        tfidf_dict = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "    # Generate embeddings for each tweet\n",
    "    tweet_embeddings = []\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet\n",
    "        if aggregation_method == 'weighted_average':\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            token_weights = [tfidf_scores[word_index[token], tfidf_dict[token]] for token in tokens if token in word_index and token in tfidf_dict]\n",
    "            if len(token_indices) > 0:\n",
    "                tweet_embedding = np.average([embedding_matrix[idx] for idx in token_indices], axis=0, weights=token_weights)\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        else:\n",
    "            token_indices = [word_index[token] for token in tokens if token in word_index]\n",
    "            if len(token_indices) > 0:\n",
    "                if aggregation_method == 'mean':\n",
    "                    tweet_embedding = np.mean([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                elif aggregation_method == 'sum':\n",
    "                    tweet_embedding = np.sum([embedding_matrix[idx] for idx in token_indices], axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid aggregation method\")\n",
    "            else:\n",
    "                tweet_embedding = np.zeros(embedding_dim)\n",
    "        tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "    return np.array(tweet_embeddings)\n",
    "\n",
    "# Example usage:\n",
    "fasttext_file = '/Users/antanas/GitRepo/SentPred/models/FastText/wiki-news-300d-1M.vec'\n",
    "tweet_embeddings = FastText_embed(t_tweets['TweetText'], fasttext_file, aggregation_method='mean')\n",
    "# You can change 'mean' to 'sum' or 'weighted_average' as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
